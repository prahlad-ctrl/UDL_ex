{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Notebook 11.2: Residual Networks**\n",
        "\n",
        "This notebook adapts the networks for MNIST1D to use residual connections.\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TODO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
        "\n",
        "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n",
        "\n"
      ],
      "metadata": {
        "id": "t9vk9Elugvmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this if you're in a Colab to install MNIST 1D repository\n",
        "!pip install git+https://github.com/greydanus/mnist1d"
      ],
      "metadata": {
        "id": "D5yLObtZCi9J",
        "outputId": "e554a6d4-ecd1-4eac-d924-38187abb16cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/greydanus/mnist1d\n",
            "  Cloning https://github.com/greydanus/mnist1d to /tmp/pip-req-build-7lxr2eex\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/greydanus/mnist1d /tmp/pip-req-build-7lxr2eex\n",
            "  Resolved https://github.com/greydanus/mnist1d to commit 7878d96082abd200c546a07a4101fa90b30fdf7e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from mnist1d==0.0.2.post16) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mnist1d==0.0.2.post16) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mnist1d==0.0.2.post16) (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from mnist1d==0.0.2.post16) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mnist1d==0.0.2.post16) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mnist1d==0.0.2.post16) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mnist1d==0.0.2.post16) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mnist1d==0.0.2.post16) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mnist1d==0.0.2.post16) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mnist1d==0.0.2.post16) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mnist1d==0.0.2.post16) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mnist1d==0.0.2.post16) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->mnist1d==0.0.2.post16) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->mnist1d==0.0.2.post16) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->mnist1d==0.0.2.post16) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->mnist1d==0.0.2.post16) (2026.1.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mnist1d==0.0.2.post16) (1.17.0)\n",
            "Building wheels for collected packages: mnist1d\n",
            "  Building wheel for mnist1d (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mnist1d: filename=mnist1d-0.0.2.post16-py3-none-any.whl size=14663 sha256=e3afa40ae2c5432881ad75beb991b6924290bb0b4014f67fbb68d6d36581ff48\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t9yw0455/wheels/18/40/b6/29381fee9b4c80fdbc304d52bb065a7286bbcca5ca2b8737c0\n",
            "Successfully built mnist1d\n",
            "Installing collected packages: mnist1d\n",
            "Successfully installed mnist1d-0.0.2.post16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import matplotlib.pyplot as plt\n",
        "import mnist1d\n",
        "import random"
      ],
      "metadata": {
        "id": "YrXWAH7sUWvU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = mnist1d.data.get_dataset_args()\n",
        "data = mnist1d.data.get_dataset(args, path='./mnist1d_data.pkl', download=False, regenerate=False)\n",
        "\n",
        "# The training and test input and outputs are in\n",
        "# data['x'], data['y'], data['x_test'], and data['y_test']\n",
        "print(\"Examples in training set: {}\".format(len(data['y'])))\n",
        "print(\"Examples in test set: {}\".format(len(data['y_test'])))\n",
        "print(\"Length of each example: {}\".format(data['x'].shape[-1]))"
      ],
      "metadata": {
        "id": "twI72ZCrCt5z",
        "outputId": "20ea9c1c-e3ba-4759-f998-a44d6b6a695a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did or could not load data from ./mnist1d_data.pkl. Rebuilding dataset...\n",
            "Examples in training set: 4000\n",
            "Examples in test set: 1000\n",
            "Length of each example: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the data\n",
        "train_data_x = data['x'].transpose()\n",
        "train_data_y = data['y']\n",
        "val_data_x = data['x_test'].transpose()\n",
        "val_data_y = data['y_test']\n",
        "# Print out sizes\n",
        "print(\"Train data: %d examples (columns), each of which has %d dimensions (rows)\"%((train_data_x.shape[1],train_data_x.shape[0])))\n",
        "print(\"Validation data: %d examples (columns), each of which has %d dimensions (rows)\"%((val_data_x.shape[1],val_data_x.shape[0])))"
      ],
      "metadata": {
        "id": "8bKADvLHbiV5",
        "outputId": "0d726c42-bebc-48a4-d8a0-1285147b65fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data: 4000 examples (columns), each of which has 40 dimensions (rows)\n",
            "Validation data: 1000 examples (columns), each of which has 40 dimensions (rows)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the network"
      ],
      "metadata": {
        "id": "_sFvRDGrl4qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There are 40 input dimensions and 10 output dimensions for this data\n",
        "# The inputs correspond to the 40 offsets in the MNIST1D template.\n",
        "D_i = 40\n",
        "# The outputs correspond to the 10 digits\n",
        "D_o = 10\n",
        "\n",
        "\n",
        "# We will adapt this model to have residual connections around the linear layers\n",
        "# This is the same model we used in practical 8.1, but we can't use the sequential\n",
        "# class for residual networks (which aren't strictly sequential).  Hence, I've rewritten\n",
        "# it as a model that inherits from a base class\n",
        "\n",
        "class ResidualNetwork(torch.nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_size=100):\n",
        "    super(ResidualNetwork, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.linear4 = nn.Linear(hidden_size, output_size)\n",
        "    print(\"Initialized MLPBase model with {} parameters\".format(self.count_params()))\n",
        "\n",
        "  def count_params(self):\n",
        "    return sum([p.view(-1).shape[0] for p in self.parameters()])\n",
        "\n",
        "# TODO -- Add residual connections to this model\n",
        "# The order of operations within each block should similar to figure 11.5b\n",
        "# ie., linear1 first, ReLU+linear2 in first residual block, ReLU+linear3 in second residual block), linear4 at end\n",
        "# Replace this function\n",
        "  def forward(self, x):\n",
        "    h = self.linear1(x)\n",
        "    h = h + self.linear2(h.relu())\n",
        "    h = h + self.linear3(h.relu())\n",
        "    out = self.linear4(h)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "FslroPJJffrh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# He initialization of weights\n",
        "def weights_init(layer_in):\n",
        "  if isinstance(layer_in, nn.Linear):\n",
        "    nn.init.kaiming_uniform_(layer_in.weight)\n",
        "    layer_in.bias.data.fill_(0.0)"
      ],
      "metadata": {
        "id": "YgLaex1pfhqz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the model\n",
        "model = ResidualNetwork(40, 10)\n",
        "\n",
        "# choose cross entropy loss function (equation 5.24 in the loss notes)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "# construct SGD optimizer and initialize learning rate and momentum\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.05, momentum=0.9)\n",
        "# object that decreases learning rate by half every 20 epochs\n",
        "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "# convert data to torch tensors\n",
        "x_train = torch.tensor(train_data_x.transpose().astype('float32'))\n",
        "y_train = torch.tensor(train_data_y.astype('long'))\n",
        "x_val= torch.tensor(val_data_x.transpose().astype('float32'))\n",
        "y_val = torch.tensor(val_data_y.astype('long'))\n",
        "\n",
        "# load the data into a class that creates the batches\n",
        "data_loader = DataLoader(TensorDataset(x_train,y_train), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))\n",
        "\n",
        "# Initialize model weights\n",
        "model.apply(weights_init)\n",
        "\n",
        "# loop over the dataset n_epoch times\n",
        "n_epoch = 250\n",
        "# store the loss and the % correct at each epoch\n",
        "losses_train = np.zeros((n_epoch))\n",
        "errors_train = np.zeros((n_epoch))\n",
        "losses_val = np.zeros((n_epoch))\n",
        "errors_val = np.zeros((n_epoch))\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  # loop over batches\n",
        "  for i, data in enumerate(data_loader):\n",
        "    # retrieve inputs and labels for this batch\n",
        "    x_batch, y_batch = data\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass -- calculate model output\n",
        "    pred = model(x_batch)\n",
        "    # compute the loss\n",
        "    loss = loss_function(pred, y_batch)\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    # SGD update\n",
        "    optimizer.step()\n",
        "\n",
        "  # Run whole dataset to get statistics -- normally wouldn't do this\n",
        "  pred_train = model(x_train)\n",
        "  pred_val = model(x_val)\n",
        "  _, predicted_train_class = torch.max(pred_train.data, 1)\n",
        "  _, predicted_val_class = torch.max(pred_val.data, 1)\n",
        "  errors_train[epoch] = 100 - 100 * (predicted_train_class == y_train).float().sum() / len(y_train)\n",
        "  errors_val[epoch]= 100 - 100 * (predicted_val_class == y_val).float().sum() / len(y_val)\n",
        "  losses_train[epoch] = loss_function(pred_train, y_train).item()\n",
        "  losses_val[epoch]= loss_function(pred_val, y_val).item()\n",
        "  print(f'Epoch {epoch:5d}, train loss {losses_train[epoch]:.6f}, train error {errors_train[epoch]:3.2f},  val loss {losses_val[epoch]:.6f}, percent error {errors_val[epoch]:3.2f}')\n",
        "\n",
        "  # tell scheduler to consider updating learning rate\n",
        "  scheduler.step()"
      ],
      "metadata": {
        "id": "NYw8I_3mmX5c",
        "outputId": "1066e261-f2fe-490f-f612-ccf659177c82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized MLPBase model with 25310 parameters\n",
            "Epoch     0, train loss 1.598676, train error 59.42,  val loss 1.745014, percent error 66.50\n",
            "Epoch     1, train loss 1.186977, train error 44.05,  val loss 1.343320, percent error 51.60\n",
            "Epoch     2, train loss 1.014463, train error 39.17,  val loss 1.156267, percent error 44.20\n",
            "Epoch     3, train loss 0.963492, train error 36.65,  val loss 1.165302, percent error 43.70\n",
            "Epoch     4, train loss 0.805440, train error 31.00,  val loss 1.076072, percent error 41.40\n",
            "Epoch     5, train loss 0.740150, train error 28.45,  val loss 1.140142, percent error 40.20\n",
            "Epoch     6, train loss 0.733668, train error 27.85,  val loss 1.146797, percent error 41.50\n",
            "Epoch     7, train loss 0.650138, train error 24.93,  val loss 1.157754, percent error 41.00\n",
            "Epoch     8, train loss 0.613727, train error 23.62,  val loss 1.171456, percent error 39.80\n",
            "Epoch     9, train loss 0.465961, train error 17.47,  val loss 1.066695, percent error 35.80\n",
            "Epoch    10, train loss 0.513928, train error 19.57,  val loss 1.195190, percent error 38.80\n",
            "Epoch    11, train loss 0.446870, train error 17.43,  val loss 1.174069, percent error 37.80\n",
            "Epoch    12, train loss 0.393549, train error 15.62,  val loss 1.259791, percent error 39.80\n",
            "Epoch    13, train loss 0.513867, train error 18.80,  val loss 1.476481, percent error 40.00\n",
            "Epoch    14, train loss 0.470772, train error 16.82,  val loss 1.271324, percent error 38.40\n",
            "Epoch    15, train loss 0.342945, train error 12.43,  val loss 1.255250, percent error 35.80\n",
            "Epoch    16, train loss 0.308278, train error 11.80,  val loss 1.395069, percent error 35.70\n",
            "Epoch    17, train loss 0.292552, train error 10.82,  val loss 1.476547, percent error 35.90\n",
            "Epoch    18, train loss 0.260912, train error 9.55,  val loss 1.407480, percent error 34.00\n",
            "Epoch    19, train loss 0.234287, train error 9.03,  val loss 1.372884, percent error 33.80\n",
            "Epoch    20, train loss 0.084933, train error 2.55,  val loss 1.255883, percent error 32.10\n",
            "Epoch    21, train loss 0.041557, train error 0.60,  val loss 1.236562, percent error 29.70\n",
            "Epoch    22, train loss 0.024006, train error 0.05,  val loss 1.210406, percent error 29.40\n",
            "Epoch    23, train loss 0.017778, train error 0.00,  val loss 1.252579, percent error 29.10\n",
            "Epoch    24, train loss 0.013798, train error 0.00,  val loss 1.272543, percent error 28.50\n",
            "Epoch    25, train loss 0.011632, train error 0.00,  val loss 1.290608, percent error 28.10\n",
            "Epoch    26, train loss 0.010066, train error 0.00,  val loss 1.316757, percent error 27.70\n",
            "Epoch    27, train loss 0.008921, train error 0.00,  val loss 1.332888, percent error 28.10\n",
            "Epoch    28, train loss 0.007998, train error 0.00,  val loss 1.350785, percent error 28.10\n",
            "Epoch    29, train loss 0.007212, train error 0.00,  val loss 1.371793, percent error 27.90\n",
            "Epoch    30, train loss 0.006584, train error 0.00,  val loss 1.384021, percent error 28.00\n",
            "Epoch    31, train loss 0.006083, train error 0.00,  val loss 1.398509, percent error 28.00\n",
            "Epoch    32, train loss 0.005634, train error 0.00,  val loss 1.413602, percent error 27.80\n",
            "Epoch    33, train loss 0.005235, train error 0.00,  val loss 1.427008, percent error 27.90\n",
            "Epoch    34, train loss 0.004889, train error 0.00,  val loss 1.437587, percent error 28.10\n",
            "Epoch    35, train loss 0.004589, train error 0.00,  val loss 1.451438, percent error 27.60\n",
            "Epoch    36, train loss 0.004320, train error 0.00,  val loss 1.458877, percent error 27.80\n",
            "Epoch    37, train loss 0.004074, train error 0.00,  val loss 1.472562, percent error 28.10\n",
            "Epoch    38, train loss 0.003864, train error 0.00,  val loss 1.480430, percent error 27.80\n",
            "Epoch    39, train loss 0.003662, train error 0.00,  val loss 1.492504, percent error 28.20\n",
            "Epoch    40, train loss 0.003566, train error 0.00,  val loss 1.497469, percent error 28.10\n",
            "Epoch    41, train loss 0.003481, train error 0.00,  val loss 1.502223, percent error 28.10\n",
            "Epoch    42, train loss 0.003403, train error 0.00,  val loss 1.506342, percent error 28.20\n",
            "Epoch    43, train loss 0.003324, train error 0.00,  val loss 1.511063, percent error 28.30\n",
            "Epoch    44, train loss 0.003250, train error 0.00,  val loss 1.514606, percent error 28.10\n",
            "Epoch    45, train loss 0.003180, train error 0.00,  val loss 1.518420, percent error 28.30\n",
            "Epoch    46, train loss 0.003113, train error 0.00,  val loss 1.522962, percent error 28.20\n",
            "Epoch    47, train loss 0.003048, train error 0.00,  val loss 1.526971, percent error 28.30\n",
            "Epoch    48, train loss 0.002985, train error 0.00,  val loss 1.530451, percent error 28.30\n",
            "Epoch    49, train loss 0.002926, train error 0.00,  val loss 1.534761, percent error 28.20\n",
            "Epoch    50, train loss 0.002868, train error 0.00,  val loss 1.537636, percent error 28.10\n",
            "Epoch    51, train loss 0.002812, train error 0.00,  val loss 1.542090, percent error 28.10\n",
            "Epoch    52, train loss 0.002758, train error 0.00,  val loss 1.545822, percent error 28.20\n",
            "Epoch    53, train loss 0.002705, train error 0.00,  val loss 1.549189, percent error 28.30\n",
            "Epoch    54, train loss 0.002656, train error 0.00,  val loss 1.553064, percent error 28.30\n",
            "Epoch    55, train loss 0.002607, train error 0.00,  val loss 1.556652, percent error 28.20\n",
            "Epoch    56, train loss 0.002561, train error 0.00,  val loss 1.559413, percent error 28.10\n",
            "Epoch    57, train loss 0.002515, train error 0.00,  val loss 1.562761, percent error 28.40\n",
            "Epoch    58, train loss 0.002472, train error 0.00,  val loss 1.566580, percent error 28.30\n",
            "Epoch    59, train loss 0.002429, train error 0.00,  val loss 1.569108, percent error 28.10\n",
            "Epoch    60, train loss 0.002408, train error 0.00,  val loss 1.571178, percent error 28.20\n",
            "Epoch    61, train loss 0.002387, train error 0.00,  val loss 1.572327, percent error 28.20\n",
            "Epoch    62, train loss 0.002368, train error 0.00,  val loss 1.574327, percent error 28.20\n",
            "Epoch    63, train loss 0.002348, train error 0.00,  val loss 1.575535, percent error 28.30\n",
            "Epoch    64, train loss 0.002329, train error 0.00,  val loss 1.577031, percent error 28.30\n",
            "Epoch    65, train loss 0.002310, train error 0.00,  val loss 1.579100, percent error 28.10\n",
            "Epoch    66, train loss 0.002291, train error 0.00,  val loss 1.580367, percent error 28.20\n",
            "Epoch    67, train loss 0.002273, train error 0.00,  val loss 1.581446, percent error 28.20\n",
            "Epoch    68, train loss 0.002255, train error 0.00,  val loss 1.583440, percent error 28.20\n",
            "Epoch    69, train loss 0.002237, train error 0.00,  val loss 1.585319, percent error 28.30\n",
            "Epoch    70, train loss 0.002219, train error 0.00,  val loss 1.586457, percent error 28.20\n",
            "Epoch    71, train loss 0.002202, train error 0.00,  val loss 1.587943, percent error 28.20\n",
            "Epoch    72, train loss 0.002185, train error 0.00,  val loss 1.589730, percent error 28.20\n",
            "Epoch    73, train loss 0.002168, train error 0.00,  val loss 1.591151, percent error 28.20\n",
            "Epoch    74, train loss 0.002151, train error 0.00,  val loss 1.592727, percent error 28.30\n",
            "Epoch    75, train loss 0.002135, train error 0.00,  val loss 1.593773, percent error 28.20\n",
            "Epoch    76, train loss 0.002119, train error 0.00,  val loss 1.595361, percent error 28.30\n",
            "Epoch    77, train loss 0.002103, train error 0.00,  val loss 1.596721, percent error 28.40\n",
            "Epoch    78, train loss 0.002087, train error 0.00,  val loss 1.598073, percent error 28.30\n",
            "Epoch    79, train loss 0.002072, train error 0.00,  val loss 1.599688, percent error 28.30\n",
            "Epoch    80, train loss 0.002064, train error 0.00,  val loss 1.600292, percent error 28.30\n",
            "Epoch    81, train loss 0.002057, train error 0.00,  val loss 1.601264, percent error 28.30\n",
            "Epoch    82, train loss 0.002049, train error 0.00,  val loss 1.601850, percent error 28.30\n",
            "Epoch    83, train loss 0.002041, train error 0.00,  val loss 1.602669, percent error 28.30\n",
            "Epoch    84, train loss 0.002034, train error 0.00,  val loss 1.603130, percent error 28.30\n",
            "Epoch    85, train loss 0.002027, train error 0.00,  val loss 1.604043, percent error 28.30\n",
            "Epoch    86, train loss 0.002019, train error 0.00,  val loss 1.604483, percent error 28.30\n",
            "Epoch    87, train loss 0.002012, train error 0.00,  val loss 1.605285, percent error 28.30\n",
            "Epoch    88, train loss 0.002005, train error 0.00,  val loss 1.606112, percent error 28.30\n",
            "Epoch    89, train loss 0.001998, train error 0.00,  val loss 1.606747, percent error 28.30\n",
            "Epoch    90, train loss 0.001991, train error 0.00,  val loss 1.607304, percent error 28.30\n",
            "Epoch    91, train loss 0.001983, train error 0.00,  val loss 1.608041, percent error 28.30\n",
            "Epoch    92, train loss 0.001977, train error 0.00,  val loss 1.608731, percent error 28.30\n",
            "Epoch    93, train loss 0.001970, train error 0.00,  val loss 1.609333, percent error 28.30\n",
            "Epoch    94, train loss 0.001963, train error 0.00,  val loss 1.610006, percent error 28.30\n",
            "Epoch    95, train loss 0.001956, train error 0.00,  val loss 1.610917, percent error 28.30\n",
            "Epoch    96, train loss 0.001949, train error 0.00,  val loss 1.611443, percent error 28.30\n",
            "Epoch    97, train loss 0.001942, train error 0.00,  val loss 1.612095, percent error 28.30\n",
            "Epoch    98, train loss 0.001935, train error 0.00,  val loss 1.612882, percent error 28.30\n",
            "Epoch    99, train loss 0.001929, train error 0.00,  val loss 1.613593, percent error 28.30\n",
            "Epoch   100, train loss 0.001925, train error 0.00,  val loss 1.613805, percent error 28.30\n",
            "Epoch   101, train loss 0.001922, train error 0.00,  val loss 1.614161, percent error 28.30\n",
            "Epoch   102, train loss 0.001919, train error 0.00,  val loss 1.614502, percent error 28.30\n",
            "Epoch   103, train loss 0.001915, train error 0.00,  val loss 1.614811, percent error 28.30\n",
            "Epoch   104, train loss 0.001912, train error 0.00,  val loss 1.615203, percent error 28.30\n",
            "Epoch   105, train loss 0.001909, train error 0.00,  val loss 1.615452, percent error 28.30\n",
            "Epoch   106, train loss 0.001905, train error 0.00,  val loss 1.615731, percent error 28.30\n",
            "Epoch   107, train loss 0.001902, train error 0.00,  val loss 1.616115, percent error 28.30\n",
            "Epoch   108, train loss 0.001899, train error 0.00,  val loss 1.616421, percent error 28.30\n",
            "Epoch   109, train loss 0.001896, train error 0.00,  val loss 1.616795, percent error 28.30\n",
            "Epoch   110, train loss 0.001892, train error 0.00,  val loss 1.617200, percent error 28.30\n",
            "Epoch   111, train loss 0.001889, train error 0.00,  val loss 1.617467, percent error 28.30\n",
            "Epoch   112, train loss 0.001886, train error 0.00,  val loss 1.617859, percent error 28.30\n",
            "Epoch   113, train loss 0.001883, train error 0.00,  val loss 1.617985, percent error 28.30\n",
            "Epoch   114, train loss 0.001880, train error 0.00,  val loss 1.618430, percent error 28.30\n",
            "Epoch   115, train loss 0.001876, train error 0.00,  val loss 1.618624, percent error 28.30\n",
            "Epoch   116, train loss 0.001873, train error 0.00,  val loss 1.619062, percent error 28.30\n",
            "Epoch   117, train loss 0.001870, train error 0.00,  val loss 1.619306, percent error 28.30\n",
            "Epoch   118, train loss 0.001867, train error 0.00,  val loss 1.619727, percent error 28.30\n",
            "Epoch   119, train loss 0.001864, train error 0.00,  val loss 1.620208, percent error 28.30\n",
            "Epoch   120, train loss 0.001862, train error 0.00,  val loss 1.620289, percent error 28.30\n",
            "Epoch   121, train loss 0.001861, train error 0.00,  val loss 1.620435, percent error 28.30\n",
            "Epoch   122, train loss 0.001859, train error 0.00,  val loss 1.620569, percent error 28.30\n",
            "Epoch   123, train loss 0.001858, train error 0.00,  val loss 1.620767, percent error 28.30\n",
            "Epoch   124, train loss 0.001856, train error 0.00,  val loss 1.620969, percent error 28.30\n",
            "Epoch   125, train loss 0.001855, train error 0.00,  val loss 1.621101, percent error 28.40\n",
            "Epoch   126, train loss 0.001853, train error 0.00,  val loss 1.621243, percent error 28.40\n",
            "Epoch   127, train loss 0.001851, train error 0.00,  val loss 1.621442, percent error 28.30\n",
            "Epoch   128, train loss 0.001850, train error 0.00,  val loss 1.621556, percent error 28.40\n",
            "Epoch   129, train loss 0.001848, train error 0.00,  val loss 1.621736, percent error 28.40\n",
            "Epoch   130, train loss 0.001847, train error 0.00,  val loss 1.621898, percent error 28.40\n",
            "Epoch   131, train loss 0.001845, train error 0.00,  val loss 1.622051, percent error 28.40\n",
            "Epoch   132, train loss 0.001844, train error 0.00,  val loss 1.622206, percent error 28.40\n",
            "Epoch   133, train loss 0.001842, train error 0.00,  val loss 1.622347, percent error 28.40\n",
            "Epoch   134, train loss 0.001841, train error 0.00,  val loss 1.622522, percent error 28.40\n",
            "Epoch   135, train loss 0.001839, train error 0.00,  val loss 1.622676, percent error 28.40\n",
            "Epoch   136, train loss 0.001838, train error 0.00,  val loss 1.622829, percent error 28.40\n",
            "Epoch   137, train loss 0.001836, train error 0.00,  val loss 1.622985, percent error 28.40\n",
            "Epoch   138, train loss 0.001835, train error 0.00,  val loss 1.623188, percent error 28.40\n",
            "Epoch   139, train loss 0.001833, train error 0.00,  val loss 1.623340, percent error 28.40\n",
            "Epoch   140, train loss 0.001832, train error 0.00,  val loss 1.623396, percent error 28.40\n",
            "Epoch   141, train loss 0.001832, train error 0.00,  val loss 1.623479, percent error 28.40\n",
            "Epoch   142, train loss 0.001831, train error 0.00,  val loss 1.623547, percent error 28.40\n",
            "Epoch   143, train loss 0.001830, train error 0.00,  val loss 1.623638, percent error 28.40\n",
            "Epoch   144, train loss 0.001829, train error 0.00,  val loss 1.623715, percent error 28.40\n",
            "Epoch   145, train loss 0.001829, train error 0.00,  val loss 1.623788, percent error 28.40\n",
            "Epoch   146, train loss 0.001828, train error 0.00,  val loss 1.623877, percent error 28.40\n",
            "Epoch   147, train loss 0.001827, train error 0.00,  val loss 1.623959, percent error 28.40\n",
            "Epoch   148, train loss 0.001826, train error 0.00,  val loss 1.624013, percent error 28.40\n",
            "Epoch   149, train loss 0.001826, train error 0.00,  val loss 1.624118, percent error 28.40\n",
            "Epoch   150, train loss 0.001825, train error 0.00,  val loss 1.624193, percent error 28.40\n",
            "Epoch   151, train loss 0.001824, train error 0.00,  val loss 1.624262, percent error 28.40\n",
            "Epoch   152, train loss 0.001823, train error 0.00,  val loss 1.624364, percent error 28.40\n",
            "Epoch   153, train loss 0.001823, train error 0.00,  val loss 1.624420, percent error 28.40\n",
            "Epoch   154, train loss 0.001822, train error 0.00,  val loss 1.624510, percent error 28.40\n",
            "Epoch   155, train loss 0.001821, train error 0.00,  val loss 1.624612, percent error 28.40\n",
            "Epoch   156, train loss 0.001820, train error 0.00,  val loss 1.624671, percent error 28.40\n",
            "Epoch   157, train loss 0.001820, train error 0.00,  val loss 1.624745, percent error 28.40\n",
            "Epoch   158, train loss 0.001819, train error 0.00,  val loss 1.624818, percent error 28.40\n",
            "Epoch   159, train loss 0.001818, train error 0.00,  val loss 1.624890, percent error 28.40\n",
            "Epoch   160, train loss 0.001818, train error 0.00,  val loss 1.624946, percent error 28.40\n",
            "Epoch   161, train loss 0.001817, train error 0.00,  val loss 1.624979, percent error 28.40\n",
            "Epoch   162, train loss 0.001817, train error 0.00,  val loss 1.625029, percent error 28.40\n",
            "Epoch   163, train loss 0.001817, train error 0.00,  val loss 1.625064, percent error 28.40\n",
            "Epoch   164, train loss 0.001816, train error 0.00,  val loss 1.625106, percent error 28.40\n",
            "Epoch   165, train loss 0.001816, train error 0.00,  val loss 1.625139, percent error 28.40\n",
            "Epoch   166, train loss 0.001815, train error 0.00,  val loss 1.625177, percent error 28.40\n",
            "Epoch   167, train loss 0.001815, train error 0.00,  val loss 1.625218, percent error 28.40\n",
            "Epoch   168, train loss 0.001815, train error 0.00,  val loss 1.625263, percent error 28.40\n",
            "Epoch   169, train loss 0.001814, train error 0.00,  val loss 1.625297, percent error 28.40\n",
            "Epoch   170, train loss 0.001814, train error 0.00,  val loss 1.625346, percent error 28.40\n",
            "Epoch   171, train loss 0.001814, train error 0.00,  val loss 1.625385, percent error 28.40\n",
            "Epoch   172, train loss 0.001813, train error 0.00,  val loss 1.625424, percent error 28.40\n",
            "Epoch   173, train loss 0.001813, train error 0.00,  val loss 1.625458, percent error 28.40\n",
            "Epoch   174, train loss 0.001812, train error 0.00,  val loss 1.625490, percent error 28.40\n",
            "Epoch   175, train loss 0.001812, train error 0.00,  val loss 1.625538, percent error 28.40\n",
            "Epoch   176, train loss 0.001812, train error 0.00,  val loss 1.625586, percent error 28.40\n",
            "Epoch   177, train loss 0.001811, train error 0.00,  val loss 1.625618, percent error 28.40\n",
            "Epoch   178, train loss 0.001811, train error 0.00,  val loss 1.625657, percent error 28.40\n",
            "Epoch   179, train loss 0.001811, train error 0.00,  val loss 1.625689, percent error 28.40\n",
            "Epoch   180, train loss 0.001810, train error 0.00,  val loss 1.625713, percent error 28.40\n",
            "Epoch   181, train loss 0.001810, train error 0.00,  val loss 1.625731, percent error 28.40\n",
            "Epoch   182, train loss 0.001810, train error 0.00,  val loss 1.625750, percent error 28.40\n",
            "Epoch   183, train loss 0.001810, train error 0.00,  val loss 1.625773, percent error 28.40\n",
            "Epoch   184, train loss 0.001810, train error 0.00,  val loss 1.625789, percent error 28.40\n",
            "Epoch   185, train loss 0.001810, train error 0.00,  val loss 1.625813, percent error 28.40\n",
            "Epoch   186, train loss 0.001809, train error 0.00,  val loss 1.625837, percent error 28.40\n",
            "Epoch   187, train loss 0.001809, train error 0.00,  val loss 1.625852, percent error 28.40\n",
            "Epoch   188, train loss 0.001809, train error 0.00,  val loss 1.625873, percent error 28.40\n",
            "Epoch   189, train loss 0.001809, train error 0.00,  val loss 1.625891, percent error 28.40\n",
            "Epoch   190, train loss 0.001809, train error 0.00,  val loss 1.625915, percent error 28.40\n",
            "Epoch   191, train loss 0.001808, train error 0.00,  val loss 1.625929, percent error 28.40\n",
            "Epoch   192, train loss 0.001808, train error 0.00,  val loss 1.625950, percent error 28.40\n",
            "Epoch   193, train loss 0.001808, train error 0.00,  val loss 1.625970, percent error 28.40\n",
            "Epoch   194, train loss 0.001808, train error 0.00,  val loss 1.625994, percent error 28.40\n",
            "Epoch   195, train loss 0.001808, train error 0.00,  val loss 1.626014, percent error 28.40\n",
            "Epoch   196, train loss 0.001807, train error 0.00,  val loss 1.626030, percent error 28.40\n",
            "Epoch   197, train loss 0.001807, train error 0.00,  val loss 1.626042, percent error 28.40\n",
            "Epoch   198, train loss 0.001807, train error 0.00,  val loss 1.626069, percent error 28.40\n",
            "Epoch   199, train loss 0.001807, train error 0.00,  val loss 1.626091, percent error 28.40\n",
            "Epoch   200, train loss 0.001807, train error 0.00,  val loss 1.626099, percent error 28.40\n",
            "Epoch   201, train loss 0.001807, train error 0.00,  val loss 1.626108, percent error 28.40\n",
            "Epoch   202, train loss 0.001807, train error 0.00,  val loss 1.626118, percent error 28.40\n",
            "Epoch   203, train loss 0.001807, train error 0.00,  val loss 1.626130, percent error 28.40\n",
            "Epoch   204, train loss 0.001806, train error 0.00,  val loss 1.626139, percent error 28.40\n",
            "Epoch   205, train loss 0.001806, train error 0.00,  val loss 1.626146, percent error 28.40\n",
            "Epoch   206, train loss 0.001806, train error 0.00,  val loss 1.626157, percent error 28.40\n",
            "Epoch   207, train loss 0.001806, train error 0.00,  val loss 1.626166, percent error 28.40\n",
            "Epoch   208, train loss 0.001806, train error 0.00,  val loss 1.626176, percent error 28.40\n",
            "Epoch   209, train loss 0.001806, train error 0.00,  val loss 1.626186, percent error 28.40\n",
            "Epoch   210, train loss 0.001806, train error 0.00,  val loss 1.626197, percent error 28.40\n",
            "Epoch   211, train loss 0.001806, train error 0.00,  val loss 1.626206, percent error 28.40\n",
            "Epoch   212, train loss 0.001806, train error 0.00,  val loss 1.626212, percent error 28.40\n",
            "Epoch   213, train loss 0.001806, train error 0.00,  val loss 1.626226, percent error 28.40\n",
            "Epoch   214, train loss 0.001806, train error 0.00,  val loss 1.626234, percent error 28.40\n",
            "Epoch   215, train loss 0.001806, train error 0.00,  val loss 1.626243, percent error 28.40\n",
            "Epoch   216, train loss 0.001805, train error 0.00,  val loss 1.626254, percent error 28.40\n",
            "Epoch   217, train loss 0.001805, train error 0.00,  val loss 1.626263, percent error 28.40\n",
            "Epoch   218, train loss 0.001805, train error 0.00,  val loss 1.626271, percent error 28.40\n",
            "Epoch   219, train loss 0.001805, train error 0.00,  val loss 1.626284, percent error 28.40\n",
            "Epoch   220, train loss 0.001805, train error 0.00,  val loss 1.626287, percent error 28.40\n",
            "Epoch   221, train loss 0.001805, train error 0.00,  val loss 1.626291, percent error 28.40\n",
            "Epoch   222, train loss 0.001805, train error 0.00,  val loss 1.626296, percent error 28.40\n",
            "Epoch   223, train loss 0.001805, train error 0.00,  val loss 1.626301, percent error 28.40\n",
            "Epoch   224, train loss 0.001805, train error 0.00,  val loss 1.626305, percent error 28.40\n",
            "Epoch   225, train loss 0.001805, train error 0.00,  val loss 1.626309, percent error 28.40\n",
            "Epoch   226, train loss 0.001805, train error 0.00,  val loss 1.626313, percent error 28.40\n",
            "Epoch   227, train loss 0.001805, train error 0.00,  val loss 1.626317, percent error 28.40\n",
            "Epoch   228, train loss 0.001805, train error 0.00,  val loss 1.626320, percent error 28.40\n",
            "Epoch   229, train loss 0.001805, train error 0.00,  val loss 1.626324, percent error 28.40\n",
            "Epoch   230, train loss 0.001805, train error 0.00,  val loss 1.626328, percent error 28.40\n",
            "Epoch   231, train loss 0.001805, train error 0.00,  val loss 1.626332, percent error 28.40\n",
            "Epoch   232, train loss 0.001805, train error 0.00,  val loss 1.626337, percent error 28.40\n",
            "Epoch   233, train loss 0.001805, train error 0.00,  val loss 1.626342, percent error 28.40\n",
            "Epoch   234, train loss 0.001805, train error 0.00,  val loss 1.626345, percent error 28.40\n",
            "Epoch   235, train loss 0.001805, train error 0.00,  val loss 1.626349, percent error 28.40\n",
            "Epoch   236, train loss 0.001804, train error 0.00,  val loss 1.626354, percent error 28.40\n",
            "Epoch   237, train loss 0.001804, train error 0.00,  val loss 1.626358, percent error 28.40\n",
            "Epoch   238, train loss 0.001804, train error 0.00,  val loss 1.626362, percent error 28.40\n",
            "Epoch   239, train loss 0.001804, train error 0.00,  val loss 1.626366, percent error 28.40\n",
            "Epoch   240, train loss 0.001804, train error 0.00,  val loss 1.626368, percent error 28.40\n",
            "Epoch   241, train loss 0.001804, train error 0.00,  val loss 1.626368, percent error 28.40\n",
            "Epoch   242, train loss 0.001804, train error 0.00,  val loss 1.626369, percent error 28.40\n",
            "Epoch   243, train loss 0.001804, train error 0.00,  val loss 1.626371, percent error 28.40\n",
            "Epoch   244, train loss 0.001804, train error 0.00,  val loss 1.626372, percent error 28.40\n",
            "Epoch   245, train loss 0.001804, train error 0.00,  val loss 1.626373, percent error 28.40\n",
            "Epoch   246, train loss 0.001804, train error 0.00,  val loss 1.626375, percent error 28.40\n",
            "Epoch   247, train loss 0.001804, train error 0.00,  val loss 1.626375, percent error 28.40\n",
            "Epoch   248, train loss 0.001804, train error 0.00,  val loss 1.626377, percent error 28.40\n",
            "Epoch   249, train loss 0.001804, train error 0.00,  val loss 1.626378, percent error 28.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(errors_train,'r-',label='train')\n",
        "ax.plot(errors_val,'b-',label='test')\n",
        "ax.set_ylim(0,100); ax.set_xlim(0,n_epoch)\n",
        "ax.set_xlabel('Epoch'); ax.set_ylabel('Error')\n",
        "ax.set_title('TrainError %3.2f, Val Error %3.2f'%(errors_train[-1],errors_val[-1]))\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CcP_VyEmE2sv",
        "outputId": "8e2306fd-d551-453e-dd5e-75af098cbcc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary motivation of residual networks is to allow training of much deeper networks.   \n",
        "\n",
        "TODO: Try running this network with and without the residual connections.  Does adding the residual connections change the performance?"
      ],
      "metadata": {
        "id": "wMmqhmxuAx0M"
      }
    }
  ]
}